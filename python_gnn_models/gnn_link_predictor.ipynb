{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Twitch\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://graphmining.ai/datasets/ptg/twitch/EN.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[7126, 128], edge_index=[2, 77774], y=[7126])\n",
      "\n",
      "Dataset: Twitch():\n",
      "====================\n",
      "Number of graphs: 1\n",
      "Number of features: 128\n",
      "Number of classes: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Twitch(root='data/Twitch', name='EN')\n",
    "print(dataset[0])\n",
    "#\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges: 60052\n",
      "Validation edges (positive): 1766\n",
      "Validation edges (negative): 1766\n",
      "Test edges (positive): 3532\n",
      "Test edges (negative): 3532\n",
      "Data(x=[7126, 128], y=[7126], val_pos_edge_index=[2, 1766], test_pos_edge_index=[2, 3532], train_pos_edge_index=[2, 60052], train_neg_adj_mask=[7126, 7126], val_neg_edge_index=[2, 1766], test_neg_edge_index=[2, 3532])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "\n",
    "data = train_test_split_edges(dataset[0])\n",
    "\n",
    "print('Train edges:', data.train_pos_edge_index.size(1))\n",
    "print('Validation edges (positive):', data.val_pos_edge_index.size(1))\n",
    "print('Validation edges (negative):', data.val_neg_edge_index.size(1))\n",
    "print('Test edges (positive):', data.test_pos_edge_index.size(1))\n",
    "print('Test edges (negative):', data.test_neg_edge_index.size(1))\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(dataset.num_features, 64, 32).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(data.x.to(device), data.train_pos_edge_index.to(device))\n",
    "\n",
    "    pos_edge_index = data.train_pos_edge_index.to(device)\n",
    "    pos_out = model.decode(z, pos_edge_index)\n",
    "    pos_loss = criterion(pos_out, torch.ones(pos_out.size(0), device=device))\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, \n",
    "        num_nodes=data.num_nodes,\n",
    "        num_neg_samples=pos_edge_index.size(1)\n",
    "    ).to(device)\n",
    "    neg_out = model.decode(z, neg_edge_index)\n",
    "    neg_loss = criterion(neg_out, torch.zeros(neg_out.size(0), device=device))\n",
    "\n",
    "    loss = pos_loss + neg_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.x.to(device), data.train_pos_edge_index.to(device))\n",
    "    \n",
    "    pos_out = model.decode(z, pos_edge_index.to(device))\n",
    "    neg_out = model.decode(z, neg_edge_index.to(device))\n",
    "\n",
    "    pos_y = torch.ones(pos_out.size(0), device=device)\n",
    "    neg_y = torch.zeros(neg_out.size(0), device=device)\n",
    "    y = torch.cat([pos_y, neg_y])\n",
    "    pred = torch.cat([pos_out, neg_out])\n",
    "\n",
    "    loss = criterion(pred, y).item()\n",
    "    pred = pred > 0\n",
    "    acc = pred.eq(y).sum().item() / y.size(0)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.6221, Val Loss: 0.6134, Val Acc: 0.5770, Test Loss: 0.6135, Test Acc: 0.5810\n",
      "Epoch: 002, Loss: 1.1809, Val Loss: 0.6419, Val Acc: 0.5589, Test Loss: 0.6518, Test Acc: 0.5534\n",
      "Epoch: 003, Loss: 1.2422, Val Loss: 0.6205, Val Acc: 0.5603, Test Loss: 0.6250, Test Acc: 0.5617\n",
      "Epoch: 004, Loss: 1.1881, Val Loss: 0.6058, Val Acc: 0.5807, Test Loss: 0.6041, Test Acc: 0.5858\n",
      "Epoch: 005, Loss: 1.1464, Val Loss: 0.6059, Val Acc: 0.5943, Test Loss: 0.6018, Test Acc: 0.6070\n",
      "Epoch: 006, Loss: 1.1417, Val Loss: 0.6001, Val Acc: 0.6065, Test Loss: 0.5971, Test Acc: 0.6179\n",
      "Epoch: 007, Loss: 1.1275, Val Loss: 0.5894, Val Acc: 0.6104, Test Loss: 0.5895, Test Acc: 0.6230\n",
      "Epoch: 008, Loss: 1.1036, Val Loss: 0.5815, Val Acc: 0.6175, Test Loss: 0.5859, Test Acc: 0.6217\n",
      "Epoch: 009, Loss: 1.0864, Val Loss: 0.5785, Val Acc: 0.6203, Test Loss: 0.5868, Test Acc: 0.6244\n",
      "Epoch: 010, Loss: 1.0742, Val Loss: 0.5750, Val Acc: 0.6305, Test Loss: 0.5847, Test Acc: 0.6290\n",
      "Epoch: 011, Loss: 1.0602, Val Loss: 0.5701, Val Acc: 0.6450, Test Loss: 0.5788, Test Acc: 0.6401\n",
      "Epoch: 012, Loss: 1.0479, Val Loss: 0.5667, Val Acc: 0.6574, Test Loss: 0.5733, Test Acc: 0.6535\n",
      "Epoch: 013, Loss: 1.0348, Val Loss: 0.5653, Val Acc: 0.6617, Test Loss: 0.5699, Test Acc: 0.6601\n",
      "Epoch: 014, Loss: 1.0306, Val Loss: 0.5639, Val Acc: 0.6636, Test Loss: 0.5674, Test Acc: 0.6649\n",
      "Epoch: 015, Loss: 1.0249, Val Loss: 0.5613, Val Acc: 0.6662, Test Loss: 0.5648, Test Acc: 0.6673\n",
      "Epoch: 016, Loss: 1.0202, Val Loss: 0.5575, Val Acc: 0.6670, Test Loss: 0.5620, Test Acc: 0.6675\n",
      "Epoch: 017, Loss: 1.0156, Val Loss: 0.5534, Val Acc: 0.6750, Test Loss: 0.5593, Test Acc: 0.6714\n",
      "Epoch: 018, Loss: 1.0049, Val Loss: 0.5495, Val Acc: 0.6798, Test Loss: 0.5569, Test Acc: 0.6726\n",
      "Epoch: 019, Loss: 0.9973, Val Loss: 0.5459, Val Acc: 0.6888, Test Loss: 0.5544, Test Acc: 0.6775\n",
      "Epoch: 020, Loss: 0.9852, Val Loss: 0.5434, Val Acc: 0.6911, Test Loss: 0.5525, Test Acc: 0.6836\n",
      "Epoch: 021, Loss: 0.9822, Val Loss: 0.5417, Val Acc: 0.6925, Test Loss: 0.5515, Test Acc: 0.6863\n",
      "Epoch: 022, Loss: 0.9789, Val Loss: 0.5404, Val Acc: 0.6917, Test Loss: 0.5508, Test Acc: 0.6886\n",
      "Epoch: 023, Loss: 0.9761, Val Loss: 0.5389, Val Acc: 0.6942, Test Loss: 0.5495, Test Acc: 0.6862\n",
      "Epoch: 024, Loss: 0.9745, Val Loss: 0.5373, Val Acc: 0.6973, Test Loss: 0.5479, Test Acc: 0.6859\n",
      "Epoch: 025, Loss: 0.9726, Val Loss: 0.5359, Val Acc: 0.6973, Test Loss: 0.5462, Test Acc: 0.6837\n",
      "Epoch: 026, Loss: 0.9715, Val Loss: 0.5346, Val Acc: 0.6988, Test Loss: 0.5444, Test Acc: 0.6849\n",
      "Epoch: 027, Loss: 0.9677, Val Loss: 0.5334, Val Acc: 0.6982, Test Loss: 0.5426, Test Acc: 0.6881\n",
      "Epoch: 028, Loss: 0.9647, Val Loss: 0.5326, Val Acc: 0.6968, Test Loss: 0.5411, Test Acc: 0.6915\n",
      "Epoch: 029, Loss: 0.9627, Val Loss: 0.5321, Val Acc: 0.7010, Test Loss: 0.5401, Test Acc: 0.6920\n",
      "Epoch: 030, Loss: 0.9556, Val Loss: 0.5318, Val Acc: 0.7044, Test Loss: 0.5395, Test Acc: 0.6934\n",
      "Epoch: 031, Loss: 0.9509, Val Loss: 0.5313, Val Acc: 0.7058, Test Loss: 0.5391, Test Acc: 0.6955\n",
      "Epoch: 032, Loss: 0.9488, Val Loss: 0.5310, Val Acc: 0.7058, Test Loss: 0.5392, Test Acc: 0.6958\n",
      "Epoch: 033, Loss: 0.9497, Val Loss: 0.5310, Val Acc: 0.7053, Test Loss: 0.5394, Test Acc: 0.6958\n",
      "Epoch: 034, Loss: 0.9429, Val Loss: 0.5310, Val Acc: 0.7041, Test Loss: 0.5397, Test Acc: 0.6963\n",
      "Epoch: 035, Loss: 0.9435, Val Loss: 0.5310, Val Acc: 0.7058, Test Loss: 0.5400, Test Acc: 0.6979\n",
      "Epoch: 036, Loss: 0.9413, Val Loss: 0.5309, Val Acc: 0.7070, Test Loss: 0.5405, Test Acc: 0.6975\n",
      "Epoch: 037, Loss: 0.9392, Val Loss: 0.5304, Val Acc: 0.7081, Test Loss: 0.5410, Test Acc: 0.6958\n",
      "Epoch: 038, Loss: 0.9378, Val Loss: 0.5296, Val Acc: 0.7081, Test Loss: 0.5414, Test Acc: 0.6958\n",
      "Epoch: 039, Loss: 0.9362, Val Loss: 0.5288, Val Acc: 0.7050, Test Loss: 0.5418, Test Acc: 0.6939\n",
      "Epoch: 040, Loss: 0.9297, Val Loss: 0.5284, Val Acc: 0.7041, Test Loss: 0.5420, Test Acc: 0.6931\n",
      "Epoch: 041, Loss: 0.9366, Val Loss: 0.5283, Val Acc: 0.7053, Test Loss: 0.5421, Test Acc: 0.6937\n",
      "Epoch: 042, Loss: 0.9329, Val Loss: 0.5282, Val Acc: 0.7078, Test Loss: 0.5420, Test Acc: 0.6937\n",
      "Epoch: 043, Loss: 0.9280, Val Loss: 0.5279, Val Acc: 0.7078, Test Loss: 0.5417, Test Acc: 0.6942\n",
      "Epoch: 044, Loss: 0.9250, Val Loss: 0.5276, Val Acc: 0.7095, Test Loss: 0.5410, Test Acc: 0.6948\n",
      "Epoch: 045, Loss: 0.9217, Val Loss: 0.5274, Val Acc: 0.7089, Test Loss: 0.5403, Test Acc: 0.6965\n",
      "Epoch: 046, Loss: 0.9212, Val Loss: 0.5273, Val Acc: 0.7064, Test Loss: 0.5395, Test Acc: 0.6980\n",
      "Epoch: 047, Loss: 0.9198, Val Loss: 0.5272, Val Acc: 0.7050, Test Loss: 0.5388, Test Acc: 0.6982\n",
      "Epoch: 048, Loss: 0.9189, Val Loss: 0.5268, Val Acc: 0.7044, Test Loss: 0.5380, Test Acc: 0.6989\n",
      "Epoch: 049, Loss: 0.9186, Val Loss: 0.5261, Val Acc: 0.7027, Test Loss: 0.5370, Test Acc: 0.6999\n",
      "Epoch: 050, Loss: 0.9163, Val Loss: 0.5257, Val Acc: 0.7027, Test Loss: 0.5362, Test Acc: 0.7006\n",
      "Epoch: 051, Loss: 0.9144, Val Loss: 0.5257, Val Acc: 0.7033, Test Loss: 0.5359, Test Acc: 0.7010\n",
      "Epoch: 052, Loss: 0.9176, Val Loss: 0.5263, Val Acc: 0.7050, Test Loss: 0.5358, Test Acc: 0.7007\n",
      "Epoch: 053, Loss: 0.9131, Val Loss: 0.5270, Val Acc: 0.7064, Test Loss: 0.5358, Test Acc: 0.7016\n",
      "Epoch: 054, Loss: 0.9093, Val Loss: 0.5271, Val Acc: 0.7055, Test Loss: 0.5352, Test Acc: 0.7003\n",
      "Epoch: 055, Loss: 0.9091, Val Loss: 0.5270, Val Acc: 0.7058, Test Loss: 0.5343, Test Acc: 0.7005\n",
      "Epoch: 056, Loss: 0.9055, Val Loss: 0.5272, Val Acc: 0.7061, Test Loss: 0.5338, Test Acc: 0.7009\n",
      "Epoch: 057, Loss: 0.9084, Val Loss: 0.5276, Val Acc: 0.7075, Test Loss: 0.5337, Test Acc: 0.7007\n",
      "Epoch: 058, Loss: 0.9057, Val Loss: 0.5278, Val Acc: 0.7075, Test Loss: 0.5339, Test Acc: 0.7002\n",
      "Epoch: 059, Loss: 0.9076, Val Loss: 0.5274, Val Acc: 0.7047, Test Loss: 0.5336, Test Acc: 0.7010\n",
      "Epoch: 060, Loss: 0.9055, Val Loss: 0.5268, Val Acc: 0.7053, Test Loss: 0.5333, Test Acc: 0.7019\n",
      "Epoch: 061, Loss: 0.9029, Val Loss: 0.5263, Val Acc: 0.7050, Test Loss: 0.5328, Test Acc: 0.7041\n",
      "Epoch: 062, Loss: 0.9029, Val Loss: 0.5261, Val Acc: 0.7055, Test Loss: 0.5325, Test Acc: 0.7051\n",
      "Epoch: 063, Loss: 0.9003, Val Loss: 0.5255, Val Acc: 0.7061, Test Loss: 0.5319, Test Acc: 0.7061\n",
      "Epoch: 064, Loss: 0.9015, Val Loss: 0.5251, Val Acc: 0.7055, Test Loss: 0.5315, Test Acc: 0.7050\n",
      "Epoch: 065, Loss: 0.8978, Val Loss: 0.5247, Val Acc: 0.7061, Test Loss: 0.5313, Test Acc: 0.7061\n",
      "Epoch: 066, Loss: 0.9001, Val Loss: 0.5247, Val Acc: 0.7064, Test Loss: 0.5313, Test Acc: 0.7054\n",
      "Epoch: 067, Loss: 0.8991, Val Loss: 0.5248, Val Acc: 0.7058, Test Loss: 0.5313, Test Acc: 0.7043\n",
      "Epoch: 068, Loss: 0.8935, Val Loss: 0.5239, Val Acc: 0.7078, Test Loss: 0.5305, Test Acc: 0.7036\n",
      "Epoch: 069, Loss: 0.8927, Val Loss: 0.5237, Val Acc: 0.7067, Test Loss: 0.5303, Test Acc: 0.7029\n",
      "Epoch: 070, Loss: 0.8914, Val Loss: 0.5247, Val Acc: 0.7058, Test Loss: 0.5313, Test Acc: 0.7027\n",
      "Epoch: 071, Loss: 0.8940, Val Loss: 0.5261, Val Acc: 0.7084, Test Loss: 0.5330, Test Acc: 0.7024\n",
      "Epoch: 072, Loss: 0.8905, Val Loss: 0.5249, Val Acc: 0.7084, Test Loss: 0.5323, Test Acc: 0.7039\n",
      "Epoch: 073, Loss: 0.8920, Val Loss: 0.5243, Val Acc: 0.7078, Test Loss: 0.5316, Test Acc: 0.7031\n",
      "Epoch: 074, Loss: 0.8896, Val Loss: 0.5247, Val Acc: 0.7089, Test Loss: 0.5316, Test Acc: 0.7029\n",
      "Epoch: 075, Loss: 0.8882, Val Loss: 0.5251, Val Acc: 0.7095, Test Loss: 0.5316, Test Acc: 0.7030\n",
      "Epoch: 076, Loss: 0.8877, Val Loss: 0.5243, Val Acc: 0.7089, Test Loss: 0.5311, Test Acc: 0.7033\n",
      "Epoch: 077, Loss: 0.8879, Val Loss: 0.5245, Val Acc: 0.7109, Test Loss: 0.5314, Test Acc: 0.7054\n",
      "Epoch: 078, Loss: 0.8872, Val Loss: 0.5249, Val Acc: 0.7101, Test Loss: 0.5318, Test Acc: 0.7053\n",
      "Epoch: 079, Loss: 0.8831, Val Loss: 0.5240, Val Acc: 0.7084, Test Loss: 0.5308, Test Acc: 0.7048\n",
      "Epoch: 080, Loss: 0.8864, Val Loss: 0.5235, Val Acc: 0.7081, Test Loss: 0.5298, Test Acc: 0.7041\n",
      "Epoch: 081, Loss: 0.8824, Val Loss: 0.5239, Val Acc: 0.7106, Test Loss: 0.5298, Test Acc: 0.7044\n",
      "Epoch: 082, Loss: 0.8847, Val Loss: 0.5250, Val Acc: 0.7112, Test Loss: 0.5306, Test Acc: 0.7030\n",
      "Epoch: 083, Loss: 0.8820, Val Loss: 0.5241, Val Acc: 0.7098, Test Loss: 0.5301, Test Acc: 0.7040\n",
      "Epoch: 084, Loss: 0.8844, Val Loss: 0.5229, Val Acc: 0.7104, Test Loss: 0.5291, Test Acc: 0.7055\n",
      "Epoch: 085, Loss: 0.8817, Val Loss: 0.5239, Val Acc: 0.7115, Test Loss: 0.5295, Test Acc: 0.7060\n",
      "Epoch: 086, Loss: 0.8797, Val Loss: 0.5248, Val Acc: 0.7123, Test Loss: 0.5302, Test Acc: 0.7070\n",
      "Epoch: 087, Loss: 0.8819, Val Loss: 0.5247, Val Acc: 0.7138, Test Loss: 0.5301, Test Acc: 0.7078\n",
      "Epoch: 088, Loss: 0.8799, Val Loss: 0.5236, Val Acc: 0.7126, Test Loss: 0.5292, Test Acc: 0.7067\n",
      "Epoch: 089, Loss: 0.8767, Val Loss: 0.5237, Val Acc: 0.7115, Test Loss: 0.5291, Test Acc: 0.7053\n",
      "Epoch: 090, Loss: 0.8784, Val Loss: 0.5242, Val Acc: 0.7112, Test Loss: 0.5296, Test Acc: 0.7053\n",
      "Epoch: 091, Loss: 0.8756, Val Loss: 0.5246, Val Acc: 0.7118, Test Loss: 0.5298, Test Acc: 0.7048\n",
      "Epoch: 092, Loss: 0.8786, Val Loss: 0.5248, Val Acc: 0.7123, Test Loss: 0.5298, Test Acc: 0.7053\n",
      "Epoch: 093, Loss: 0.8779, Val Loss: 0.5239, Val Acc: 0.7126, Test Loss: 0.5292, Test Acc: 0.7058\n",
      "Epoch: 094, Loss: 0.8796, Val Loss: 0.5241, Val Acc: 0.7121, Test Loss: 0.5293, Test Acc: 0.7057\n",
      "Epoch: 095, Loss: 0.8769, Val Loss: 0.5238, Val Acc: 0.7112, Test Loss: 0.5288, Test Acc: 0.7065\n",
      "Epoch: 096, Loss: 0.8722, Val Loss: 0.5227, Val Acc: 0.7109, Test Loss: 0.5277, Test Acc: 0.7054\n",
      "Epoch: 097, Loss: 0.8735, Val Loss: 0.5225, Val Acc: 0.7104, Test Loss: 0.5278, Test Acc: 0.7043\n",
      "Epoch: 098, Loss: 0.8740, Val Loss: 0.5214, Val Acc: 0.7123, Test Loss: 0.5273, Test Acc: 0.7064\n",
      "Epoch: 099, Loss: 0.8725, Val Loss: 0.5218, Val Acc: 0.7135, Test Loss: 0.5276, Test Acc: 0.7070\n",
      "Epoch: 100, Loss: 0.8720, Val Loss: 0.5233, Val Acc: 0.7101, Test Loss: 0.5286, Test Acc: 0.7046\n",
      "Epoch: 101, Loss: 0.8748, Val Loss: 0.5215, Val Acc: 0.7106, Test Loss: 0.5272, Test Acc: 0.7072\n",
      "Epoch: 102, Loss: 0.8719, Val Loss: 0.5209, Val Acc: 0.7112, Test Loss: 0.5269, Test Acc: 0.7077\n",
      "Epoch: 103, Loss: 0.8731, Val Loss: 0.5238, Val Acc: 0.7095, Test Loss: 0.5292, Test Acc: 0.7037\n",
      "Epoch: 104, Loss: 0.8717, Val Loss: 0.5216, Val Acc: 0.7112, Test Loss: 0.5272, Test Acc: 0.7071\n",
      "Epoch: 105, Loss: 0.8725, Val Loss: 0.5216, Val Acc: 0.7109, Test Loss: 0.5271, Test Acc: 0.7072\n",
      "Epoch: 106, Loss: 0.8715, Val Loss: 0.5231, Val Acc: 0.7109, Test Loss: 0.5286, Test Acc: 0.7048\n",
      "Epoch: 107, Loss: 0.8730, Val Loss: 0.5219, Val Acc: 0.7129, Test Loss: 0.5276, Test Acc: 0.7060\n",
      "Epoch: 108, Loss: 0.8697, Val Loss: 0.5217, Val Acc: 0.7115, Test Loss: 0.5274, Test Acc: 0.7041\n",
      "Epoch: 109, Loss: 0.8683, Val Loss: 0.5229, Val Acc: 0.7101, Test Loss: 0.5286, Test Acc: 0.7027\n",
      "Epoch: 110, Loss: 0.8665, Val Loss: 0.5213, Val Acc: 0.7115, Test Loss: 0.5276, Test Acc: 0.7037\n",
      "Epoch: 111, Loss: 0.8687, Val Loss: 0.5219, Val Acc: 0.7112, Test Loss: 0.5285, Test Acc: 0.7047\n",
      "Epoch: 112, Loss: 0.8679, Val Loss: 0.5242, Val Acc: 0.7123, Test Loss: 0.5305, Test Acc: 0.7016\n",
      "Epoch: 113, Loss: 0.8658, Val Loss: 0.5213, Val Acc: 0.7115, Test Loss: 0.5280, Test Acc: 0.7039\n",
      "Epoch: 114, Loss: 0.8617, Val Loss: 0.5236, Val Acc: 0.7109, Test Loss: 0.5294, Test Acc: 0.7020\n",
      "Epoch: 115, Loss: 0.8666, Val Loss: 0.5240, Val Acc: 0.7087, Test Loss: 0.5300, Test Acc: 0.7030\n",
      "Epoch: 116, Loss: 0.8633, Val Loss: 0.5215, Val Acc: 0.7101, Test Loss: 0.5283, Test Acc: 0.7043\n",
      "Epoch: 117, Loss: 0.8668, Val Loss: 0.5257, Val Acc: 0.7112, Test Loss: 0.5313, Test Acc: 0.7007\n",
      "Epoch: 118, Loss: 0.8692, Val Loss: 0.5233, Val Acc: 0.7109, Test Loss: 0.5294, Test Acc: 0.7023\n",
      "Epoch: 119, Loss: 0.8662, Val Loss: 0.5222, Val Acc: 0.7098, Test Loss: 0.5287, Test Acc: 0.7037\n",
      "Epoch: 120, Loss: 0.8625, Val Loss: 0.5248, Val Acc: 0.7089, Test Loss: 0.5303, Test Acc: 0.7012\n",
      "Epoch: 121, Loss: 0.8644, Val Loss: 0.5226, Val Acc: 0.7081, Test Loss: 0.5280, Test Acc: 0.7055\n",
      "Epoch: 122, Loss: 0.8615, Val Loss: 0.5219, Val Acc: 0.7070, Test Loss: 0.5279, Test Acc: 0.7064\n",
      "Epoch: 123, Loss: 0.8648, Val Loss: 0.5260, Val Acc: 0.7089, Test Loss: 0.5315, Test Acc: 0.7040\n",
      "Epoch: 124, Loss: 0.8606, Val Loss: 0.5224, Val Acc: 0.7070, Test Loss: 0.5286, Test Acc: 0.7061\n",
      "Epoch: 125, Loss: 0.8640, Val Loss: 0.5228, Val Acc: 0.7075, Test Loss: 0.5294, Test Acc: 0.7039\n",
      "Epoch: 126, Loss: 0.8620, Val Loss: 0.5241, Val Acc: 0.7072, Test Loss: 0.5305, Test Acc: 0.7014\n",
      "Epoch: 127, Loss: 0.8617, Val Loss: 0.5210, Val Acc: 0.7064, Test Loss: 0.5282, Test Acc: 0.7057\n",
      "Epoch: 128, Loss: 0.8599, Val Loss: 0.5237, Val Acc: 0.7089, Test Loss: 0.5306, Test Acc: 0.7034\n",
      "Epoch: 129, Loss: 0.8574, Val Loss: 0.5210, Val Acc: 0.7050, Test Loss: 0.5292, Test Acc: 0.7051\n",
      "Epoch: 130, Loss: 0.8602, Val Loss: 0.5215, Val Acc: 0.7078, Test Loss: 0.5298, Test Acc: 0.7058\n",
      "Epoch: 131, Loss: 0.8614, Val Loss: 0.5248, Val Acc: 0.7075, Test Loss: 0.5326, Test Acc: 0.7044\n",
      "Epoch: 132, Loss: 0.8571, Val Loss: 0.5186, Val Acc: 0.7072, Test Loss: 0.5280, Test Acc: 0.7067\n",
      "Epoch: 133, Loss: 0.8656, Val Loss: 0.5259, Val Acc: 0.7024, Test Loss: 0.5342, Test Acc: 0.7003\n",
      "Epoch: 134, Loss: 0.8584, Val Loss: 0.5200, Val Acc: 0.7053, Test Loss: 0.5291, Test Acc: 0.7051\n",
      "Epoch: 135, Loss: 0.8595, Val Loss: 0.5205, Val Acc: 0.7055, Test Loss: 0.5296, Test Acc: 0.7050\n",
      "Epoch: 136, Loss: 0.8612, Val Loss: 0.5244, Val Acc: 0.7050, Test Loss: 0.5333, Test Acc: 0.7010\n",
      "Epoch: 137, Loss: 0.8606, Val Loss: 0.5185, Val Acc: 0.7055, Test Loss: 0.5288, Test Acc: 0.7030\n",
      "Epoch: 138, Loss: 0.8610, Val Loss: 0.5224, Val Acc: 0.7058, Test Loss: 0.5310, Test Acc: 0.7027\n",
      "Epoch: 139, Loss: 0.8612, Val Loss: 0.5207, Val Acc: 0.7039, Test Loss: 0.5297, Test Acc: 0.7022\n",
      "Epoch: 140, Loss: 0.8597, Val Loss: 0.5180, Val Acc: 0.7067, Test Loss: 0.5287, Test Acc: 0.7029\n",
      "Epoch: 141, Loss: 0.8566, Val Loss: 0.5230, Val Acc: 0.7036, Test Loss: 0.5327, Test Acc: 0.7003\n",
      "Epoch: 142, Loss: 0.8587, Val Loss: 0.5215, Val Acc: 0.7061, Test Loss: 0.5306, Test Acc: 0.7017\n",
      "Epoch: 143, Loss: 0.8582, Val Loss: 0.5178, Val Acc: 0.7055, Test Loss: 0.5282, Test Acc: 0.7026\n",
      "Epoch: 144, Loss: 0.8599, Val Loss: 0.5249, Val Acc: 0.7033, Test Loss: 0.5343, Test Acc: 0.6962\n",
      "Epoch: 145, Loss: 0.8589, Val Loss: 0.5189, Val Acc: 0.7078, Test Loss: 0.5292, Test Acc: 0.7039\n",
      "Epoch: 146, Loss: 0.8577, Val Loss: 0.5210, Val Acc: 0.7053, Test Loss: 0.5303, Test Acc: 0.7007\n",
      "Epoch: 147, Loss: 0.8552, Val Loss: 0.5212, Val Acc: 0.7022, Test Loss: 0.5310, Test Acc: 0.6997\n",
      "Epoch: 148, Loss: 0.8554, Val Loss: 0.5198, Val Acc: 0.7089, Test Loss: 0.5304, Test Acc: 0.7050\n",
      "Epoch: 149, Loss: 0.8529, Val Loss: 0.5244, Val Acc: 0.7036, Test Loss: 0.5331, Test Acc: 0.6989\n",
      "Epoch: 150, Loss: 0.8544, Val Loss: 0.5198, Val Acc: 0.7044, Test Loss: 0.5296, Test Acc: 0.7029\n",
      "Epoch: 151, Loss: 0.8542, Val Loss: 0.5200, Val Acc: 0.7055, Test Loss: 0.5306, Test Acc: 0.7024\n",
      "Epoch: 152, Loss: 0.8533, Val Loss: 0.5246, Val Acc: 0.7044, Test Loss: 0.5339, Test Acc: 0.6978\n",
      "Epoch: 153, Loss: 0.8563, Val Loss: 0.5188, Val Acc: 0.7064, Test Loss: 0.5287, Test Acc: 0.7020\n",
      "Epoch: 154, Loss: 0.8511, Val Loss: 0.5211, Val Acc: 0.7013, Test Loss: 0.5304, Test Acc: 0.6996\n",
      "Epoch: 155, Loss: 0.8508, Val Loss: 0.5223, Val Acc: 0.7039, Test Loss: 0.5318, Test Acc: 0.7002\n",
      "Epoch: 156, Loss: 0.8560, Val Loss: 0.5202, Val Acc: 0.7070, Test Loss: 0.5298, Test Acc: 0.7034\n",
      "Epoch: 157, Loss: 0.8526, Val Loss: 0.5232, Val Acc: 0.7041, Test Loss: 0.5315, Test Acc: 0.6985\n",
      "Epoch: 158, Loss: 0.8555, Val Loss: 0.5218, Val Acc: 0.7036, Test Loss: 0.5306, Test Acc: 0.7012\n",
      "Epoch: 159, Loss: 0.8547, Val Loss: 0.5206, Val Acc: 0.7050, Test Loss: 0.5296, Test Acc: 0.7036\n",
      "Epoch: 160, Loss: 0.8504, Val Loss: 0.5234, Val Acc: 0.7036, Test Loss: 0.5310, Test Acc: 0.6989\n",
      "Epoch: 161, Loss: 0.8506, Val Loss: 0.5211, Val Acc: 0.7053, Test Loss: 0.5288, Test Acc: 0.7031\n",
      "Epoch: 162, Loss: 0.8500, Val Loss: 0.5219, Val Acc: 0.7036, Test Loss: 0.5294, Test Acc: 0.7020\n",
      "Epoch: 163, Loss: 0.8500, Val Loss: 0.5216, Val Acc: 0.7053, Test Loss: 0.5293, Test Acc: 0.7019\n",
      "Epoch: 164, Loss: 0.8504, Val Loss: 0.5213, Val Acc: 0.7070, Test Loss: 0.5287, Test Acc: 0.7039\n",
      "Epoch: 165, Loss: 0.8522, Val Loss: 0.5234, Val Acc: 0.7039, Test Loss: 0.5309, Test Acc: 0.7017\n",
      "Epoch: 166, Loss: 0.8555, Val Loss: 0.5207, Val Acc: 0.7050, Test Loss: 0.5291, Test Acc: 0.7027\n",
      "Epoch: 167, Loss: 0.8488, Val Loss: 0.5227, Val Acc: 0.7061, Test Loss: 0.5299, Test Acc: 0.7017\n",
      "Epoch: 168, Loss: 0.8535, Val Loss: 0.5220, Val Acc: 0.7058, Test Loss: 0.5301, Test Acc: 0.7012\n",
      "Epoch: 169, Loss: 0.8499, Val Loss: 0.5206, Val Acc: 0.7050, Test Loss: 0.5304, Test Acc: 0.7014\n",
      "Epoch: 170, Loss: 0.8514, Val Loss: 0.5228, Val Acc: 0.7039, Test Loss: 0.5313, Test Acc: 0.6986\n",
      "Epoch: 171, Loss: 0.8489, Val Loss: 0.5200, Val Acc: 0.7070, Test Loss: 0.5288, Test Acc: 0.6995\n",
      "Epoch: 172, Loss: 0.8494, Val Loss: 0.5210, Val Acc: 0.7053, Test Loss: 0.5308, Test Acc: 0.6995\n",
      "Epoch: 173, Loss: 0.8512, Val Loss: 0.5208, Val Acc: 0.7030, Test Loss: 0.5304, Test Acc: 0.7002\n",
      "Epoch: 174, Loss: 0.8477, Val Loss: 0.5182, Val Acc: 0.7050, Test Loss: 0.5277, Test Acc: 0.6999\n",
      "Epoch: 175, Loss: 0.8481, Val Loss: 0.5231, Val Acc: 0.7022, Test Loss: 0.5317, Test Acc: 0.6979\n",
      "Epoch: 176, Loss: 0.8486, Val Loss: 0.5181, Val Acc: 0.7055, Test Loss: 0.5280, Test Acc: 0.7029\n",
      "Epoch: 177, Loss: 0.8461, Val Loss: 0.5214, Val Acc: 0.7044, Test Loss: 0.5308, Test Acc: 0.6969\n",
      "Epoch: 178, Loss: 0.8509, Val Loss: 0.5199, Val Acc: 0.7053, Test Loss: 0.5298, Test Acc: 0.7000\n",
      "Epoch: 179, Loss: 0.8490, Val Loss: 0.5200, Val Acc: 0.7067, Test Loss: 0.5300, Test Acc: 0.6996\n",
      "Epoch: 180, Loss: 0.8523, Val Loss: 0.5205, Val Acc: 0.7013, Test Loss: 0.5304, Test Acc: 0.6971\n",
      "Epoch: 181, Loss: 0.8505, Val Loss: 0.5176, Val Acc: 0.7064, Test Loss: 0.5286, Test Acc: 0.7023\n",
      "Epoch: 182, Loss: 0.8498, Val Loss: 0.5231, Val Acc: 0.7027, Test Loss: 0.5336, Test Acc: 0.6976\n",
      "Epoch: 183, Loss: 0.8492, Val Loss: 0.5151, Val Acc: 0.7087, Test Loss: 0.5272, Test Acc: 0.7010\n",
      "Epoch: 184, Loss: 0.8504, Val Loss: 0.5231, Val Acc: 0.6979, Test Loss: 0.5330, Test Acc: 0.6969\n",
      "Epoch: 185, Loss: 0.8510, Val Loss: 0.5171, Val Acc: 0.7055, Test Loss: 0.5288, Test Acc: 0.7000\n",
      "Epoch: 186, Loss: 0.8476, Val Loss: 0.5202, Val Acc: 0.6990, Test Loss: 0.5315, Test Acc: 0.6968\n",
      "Epoch: 187, Loss: 0.8485, Val Loss: 0.5171, Val Acc: 0.7036, Test Loss: 0.5284, Test Acc: 0.7009\n",
      "Epoch: 188, Loss: 0.8453, Val Loss: 0.5171, Val Acc: 0.7022, Test Loss: 0.5289, Test Acc: 0.6992\n",
      "Epoch: 189, Loss: 0.8491, Val Loss: 0.5231, Val Acc: 0.6937, Test Loss: 0.5336, Test Acc: 0.6962\n",
      "Epoch: 190, Loss: 0.8511, Val Loss: 0.5140, Val Acc: 0.7033, Test Loss: 0.5263, Test Acc: 0.7024\n",
      "Epoch: 191, Loss: 0.8531, Val Loss: 0.5250, Val Acc: 0.6976, Test Loss: 0.5349, Test Acc: 0.6971\n",
      "Epoch: 192, Loss: 0.8494, Val Loss: 0.5141, Val Acc: 0.7041, Test Loss: 0.5276, Test Acc: 0.7026\n",
      "Epoch: 193, Loss: 0.8509, Val Loss: 0.5227, Val Acc: 0.6939, Test Loss: 0.5329, Test Acc: 0.6956\n",
      "Epoch: 194, Loss: 0.8466, Val Loss: 0.5178, Val Acc: 0.7019, Test Loss: 0.5286, Test Acc: 0.7010\n",
      "Epoch: 195, Loss: 0.8492, Val Loss: 0.5176, Val Acc: 0.7053, Test Loss: 0.5298, Test Acc: 0.7003\n",
      "Epoch: 196, Loss: 0.8460, Val Loss: 0.5203, Val Acc: 0.6990, Test Loss: 0.5318, Test Acc: 0.6959\n",
      "Epoch: 197, Loss: 0.8481, Val Loss: 0.5151, Val Acc: 0.7050, Test Loss: 0.5272, Test Acc: 0.7013\n",
      "Epoch: 198, Loss: 0.8456, Val Loss: 0.5200, Val Acc: 0.7039, Test Loss: 0.5306, Test Acc: 0.6986\n",
      "Epoch: 199, Loss: 0.8497, Val Loss: 0.5187, Val Acc: 0.6982, Test Loss: 0.5307, Test Acc: 0.6980\n",
      "Epoch: 200, Loss: 0.8447, Val Loss: 0.5143, Val Acc: 0.7067, Test Loss: 0.5285, Test Acc: 0.6976\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    val_loss, val_acc = test(data.val_pos_edge_index, data.val_neg_edge_index)\n",
    "    test_loss, test_acc = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[7126, 128], edge_index=[2, 77774], y=[7126])\n"
     ]
    }
   ],
   "source": [
    "gData = dataset[0]\n",
    "print(gData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_count(data, node_index):\n",
    "    if node_index < 0 or node_index >= data.num_nodes:\n",
    "        raise ValueError(\"exceed the dataset\")\n",
    "    edge_index = data.edge_index\n",
    "    neighbors = edge_index[1][edge_index[0] == node_index]\n",
    "    neighbor_of_neighbor_count = 0\n",
    "    for neighbor in neighbors:\n",
    "        second_neighbors = edge_index[1][edge_index[0] == neighbor]\n",
    "        neighbor_of_neighbor_count += second_neighbors.size(0)\n",
    "    return neighbors.size(0), neighbor_of_neighbor_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6)\n",
      "(27, 801)\n",
      "(2, 339)\n",
      "(8, 133)\n",
      "(2, 81)\n",
      "(5, 833)\n",
      "(11, 194)\n",
      "(3, 13)\n",
      "(2, 13)\n",
      "(13, 1840)\n",
      "(2, 6)\n",
      "(5, 103)\n",
      "(3, 9)\n",
      "(13, 290)\n",
      "(11, 648)\n",
      "(6, 551)\n",
      "(6, 96)\n",
      "(7, 131)\n",
      "(3, 474)\n",
      "(10, 237)\n",
      "(8, 176)\n",
      "(8, 329)\n",
      "(3, 732)\n",
      "(59, 2283)\n",
      "(123, 5005)\n",
      "(2, 76)\n",
      "(92, 4983)\n",
      "(2, 5)\n",
      "(3, 80)\n",
      "(4, 392)\n",
      "(38, 1803)\n",
      "(3, 138)\n",
      "(10, 377)\n",
      "(4, 49)\n",
      "(12, 755)\n",
      "(14, 272)\n",
      "(7, 837)\n",
      "(5, 1161)\n",
      "(3, 798)\n",
      "(5, 134)\n",
      "(5, 810)\n",
      "(8, 191)\n",
      "(5, 77)\n",
      "(14, 333)\n",
      "(6, 114)\n",
      "(2, 23)\n",
      "(10, 1675)\n",
      "(2, 46)\n",
      "(11, 169)\n",
      "(6, 90)\n",
      "(2, 16)\n",
      "(3, 11)\n",
      "(7, 93)\n",
      "(10, 219)\n",
      "(3, 25)\n",
      "(15, 307)\n",
      "(15, 1519)\n",
      "(14, 284)\n",
      "(13, 1559)\n",
      "(4, 32)\n",
      "(5, 112)\n",
      "(6, 129)\n",
      "(5, 772)\n",
      "(3, 33)\n",
      "(5, 1416)\n",
      "(23, 943)\n",
      "(3, 724)\n",
      "(3, 12)\n",
      "(5, 448)\n",
      "(6, 121)\n",
      "(4, 20)\n",
      "(2, 5)\n",
      "(3, 81)\n",
      "(28, 395)\n",
      "(5, 79)\n",
      "(19, 950)\n",
      "(3, 42)\n",
      "(6, 367)\n",
      "(5, 46)\n",
      "(2, 7)\n",
      "(18, 173)\n",
      "(18, 541)\n",
      "(4, 75)\n",
      "(7, 288)\n",
      "(3, 282)\n",
      "(13, 368)\n",
      "(9, 276)\n",
      "(2, 723)\n",
      "(7, 1048)\n",
      "(6, 442)\n",
      "(3, 250)\n",
      "(11, 541)\n",
      "(2, 10)\n",
      "(155, 5411)\n",
      "(25, 2801)\n",
      "(6, 879)\n",
      "(8, 210)\n",
      "(5, 60)\n",
      "(3, 85)\n",
      "(11, 1538)\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the size graph (the neighbors of the target node, the neibhors of the neighbors node)\n",
    "for i in range(100):\n",
    "    print(get_neighbor_count(gData, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
